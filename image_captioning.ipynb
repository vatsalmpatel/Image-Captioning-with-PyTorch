{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMXp37UNzOe4Dny5nlNtc9M"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the Flickr Dataset from Kaggle"
      ],
      "metadata": {
        "id": "qy4Rv3q79XrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "O51DuZwR9XTJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "Oy9IltHu9Vw9",
        "outputId": "3f0c8e55-8bc8-41e6-8e64-22a71e4b85c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f1d0d1ad-f0c4-4c2c-9797-8c5c7b080661\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f1d0d1ad-f0c4-4c2c-9797-8c5c7b080661\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"vatsalmpatel\",\"key\":\"c9d984a8eae524b0a634bc03e0f663a2\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "poCiKt6N9Vta"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "LIvhWyAn9Vq0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "8iC__kPk9VoH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d aladdinpersson/flickr8kimagescaptions"
      ],
      "metadata": {
        "id": "BtWuDNZFuz9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d9e3c7e-74e9-4020-d6e8-a3556058d754"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading flickr8kimagescaptions.zip to /content\n",
            "100% 1.03G/1.04G [00:38<00:00, 35.7MB/s]\n",
            "100% 1.04G/1.04G [00:38<00:00, 28.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip flickr8kimagescaptions.zip"
      ],
      "metadata": {
        "id": "MVzcshVPEA8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Dataloader for loading the FLICKR 8k dataset and preparing it for training and testing"
      ],
      "metadata": {
        "id": "dDh9td2f-Im3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "A66NdVnWuWCv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_eng = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "Gih9tyHp-jwa"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "  def __init__(self,freq_thresh):\n",
        "    self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "    self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "    self.freq_thresh = freq_thresh\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.itos)\n",
        "  \n",
        "  @staticmethod\n",
        "  def tokenizer_eng(text):\n",
        "    return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "  \n",
        "  def build_vocabulary(self,sentence_list):\n",
        "    frequencies = {}\n",
        "    idx = 4\n",
        "\n",
        "    for sentence in sentence_list:\n",
        "      for word in self.tokenizer_eng(sentence):\n",
        "        if word not in frequencies:\n",
        "          frequencies[word] = 1\n",
        "        else:\n",
        "          frequencies[word] += 1\n",
        "        \n",
        "        if frequencies[word] == self.freq_thresh:\n",
        "          self.stoi[word] = idx\n",
        "          self.itos[idx] = word\n",
        "          idx = idx + 1\n",
        "  \n",
        "  def numericalize(self,text):\n",
        "    tokenized_text = self.tokenizer_eng(text)\n",
        "\n",
        "    return [\n",
        "        self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "        for token in tokenized_text\n",
        "    ]"
      ],
      "metadata": {
        "id": "uEMO4Kjp-nxL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlickrDataset(Dataset):\n",
        "  def __init__(self,root_dir,captions_file,transform = None, freq_thresh = 5):\n",
        "    self.root_dir = root_dir\n",
        "    self.df = pd.read_csv(captions_file)\n",
        "    self.transform = transform\n",
        "\n",
        "    self.imgs = self.df[\"image\"]\n",
        "    self.captions = self.df['caption']\n",
        "\n",
        "    self.vocab = Vocabulary(freq_thresh)\n",
        "    self.vocab.build_vocabulary(self.captions.tolist())\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "  \n",
        "  def __getitem__(self,index):\n",
        "    caption = self.captions[index]\n",
        "    img_id = self.imgs[index]\n",
        "    img = Image.open(os.path.join(self.root_dir,img_id)).convert(\"RGB\")\n",
        "\n",
        "    if self.transform is not None:\n",
        "      img = self.transform(img)\n",
        "    \n",
        "    numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
        "    numericalized_caption += self.vocab.numericalize(caption)\n",
        "    numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
        "\n",
        "    return img, torch.tensor(numericalized_caption)"
      ],
      "metadata": {
        "id": "Q2u7DUYyBAHB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCollate:\n",
        "  def __init__(self,pad_idx):\n",
        "    self.pad_idx = pad_idx\n",
        "  \n",
        "  def __call__(self,batch):\n",
        "    imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "    imgs = torch.cat(imgs,dim = 0)\n",
        "    targets = [item[1] for item in batch]\n",
        "    targets = pad_sequence(targets,batch_first = False, padding_value = self.pad_idx)\n",
        "\n",
        "    return imgs, targets"
      ],
      "metadata": {
        "id": "RjkfEB49CViR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loader(root,annotation_file,transform,batch_size = 32,num_workers = 8,shuffle = True, pin_memory = True):\n",
        "  dataset = FlickrDataset(root,annotation_file,transform = transform)\n",
        "\n",
        "  pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "  loader = DataLoader(\n",
        "      dataset = dataset,\n",
        "      batch_size = batch_size,\n",
        "      num_workers = num_workers,\n",
        "      pin_memory = pin_memory,\n",
        "      shuffle = shuffle,\n",
        "      collate_fn = MyCollate(pad_idx = pad_idx)\n",
        "  )\n",
        "\n",
        "  return loader, dataset"
      ],
      "metadata": {
        "id": "-cI_5WGvDB3S"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.Resize((224,224)), transforms.ToTensor()]\n",
        ")\n",
        "\n",
        "loader, dataset = get_loader(\n",
        "    \"./flickr8k/images/\",\n",
        "    \"./flickr8k/captions.txt\",\n",
        "    transform = transform\n",
        ")"
      ],
      "metadata": {
        "id": "DR7rMw-TDoSy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, (imgs, captions) in enumerate(loader):\n",
        "  print(\"Image Shape\",imgs.shape)\n",
        "  print(\"Captions Shape\",captions.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff6BgUDvERZ2",
        "outputId": "b24916c8-b932-41d3-ac6d-19298de5f4a5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Shape torch.Size([32, 3, 224, 224])\n",
            "Captions Shape torch.Size([22, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "q_6mBIjdFyl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "kjJZIE4sEcFU"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_examples(model,device,dataset):\n",
        "  transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((299, 299)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]\n",
        "    )\n",
        "  model.eval()\n",
        "  test_img1 = transform(Image.open(\"test_examples/dog.jpg\").convert(\"RGB\")).unsqueeze(\n",
        "      0\n",
        "  )\n",
        "  print(\"Example 1 CORRECT: Dog on a beach by the ocean\")\n",
        "  print(\n",
        "      \"Example 1 OUTPUT: \"\n",
        "      + \" \".join(model.caption_image(test_img1.to(device), dataset.vocab))\n",
        "  )\n",
        "  test_img2 = transform(\n",
        "      Image.open(\"test_examples/child.jpg\").convert(\"RGB\")\n",
        "  ).unsqueeze(0)\n",
        "  print(\"Example 2 CORRECT: Child holding red frisbee outdoors\")\n",
        "  print(\n",
        "      \"Example 2 OUTPUT: \"\n",
        "      + \" \".join(model.caption_image(test_img2.to(device), dataset.vocab))\n",
        "  )\n",
        "  test_img3 = transform(Image.open(\"test_examples/bus.png\").convert(\"RGB\")).unsqueeze(\n",
        "      0\n",
        "  )\n",
        "  print(\"Example 3 CORRECT: Bus driving by parked cars\")\n",
        "  print(\n",
        "      \"Example 3 OUTPUT: \"\n",
        "      + \" \".join(model.caption_image(test_img3.to(device), dataset.vocab))\n",
        "  )\n",
        "  test_img4 = transform(\n",
        "      Image.open(\"test_examples/boat.png\").convert(\"RGB\")\n",
        "  ).unsqueeze(0)\n",
        "  print(\"Example 4 CORRECT: A small boat in the ocean\")\n",
        "  print(\n",
        "      \"Example 4 OUTPUT: \"\n",
        "      + \" \".join(model.caption_image(test_img4.to(device), dataset.vocab))\n",
        "  )\n",
        "  test_img5 = transform(\n",
        "      Image.open(\"test_examples/horse.png\").convert(\"RGB\")\n",
        "  ).unsqueeze(0)\n",
        "  print(\"Example 5 CORRECT: A cowboy riding a horse in the desert\")\n",
        "  print(\n",
        "      \"Example 5 OUTPUT: \"\n",
        "      + \" \".join(model.caption_image(test_img5.to(device), dataset.vocab))\n",
        "  )\n",
        "  model.train()"
      ],
      "metadata": {
        "id": "z5vicTN9KVRI"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(state,file_name = 'model_checkpoint.pth.tar'):\n",
        "  print(\"************* Saving Model Checkpoint ***************************\")\n",
        "  torch.save(state,file_name)"
      ],
      "metadata": {
        "id": "imLC5scbKozO"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "  print(\"************ Loading Checkpoint ************\")\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "  step = checkpoint['step']\n",
        "  return step"
      ],
      "metadata": {
        "id": "hHZXyevmP9_s"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ob8NSJVNQdpj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}