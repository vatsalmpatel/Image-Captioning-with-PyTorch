{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPUy2+kRXIRb+A+DpLRfVle"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the Flickr Dataset from Kaggle"
      ],
      "metadata": {
        "id": "qy4Rv3q79XrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "O51DuZwR9XTJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.upload()"
      ],
      "metadata": {
        "id": "Oy9IltHu9Vw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "poCiKt6N9Vta"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "LIvhWyAn9Vq0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "8iC__kPk9VoH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d aladdinpersson/flickr8kimagescaptions"
      ],
      "metadata": {
        "id": "BtWuDNZFuz9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip flickr8kimagescaptions.zip"
      ],
      "metadata": {
        "id": "MVzcshVPEA8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/aladdinpersson/Machine-Learning-Collection/master/ML/Pytorch/more_advanced/image_captioning/test_examples/boat.png\n",
        "!wget https://raw.githubusercontent.com/aladdinpersson/Machine-Learning-Collection/master/ML/Pytorch/more_advanced/image_captioning/test_examples/bus.png\n",
        "!wget https://raw.githubusercontent.com/aladdinpersson/Machine-Learning-Collection/master/ML/Pytorch/more_advanced/image_captioning/test_examples/child.jpg\n",
        "!wget https://raw.githubusercontent.com/aladdinpersson/Machine-Learning-Collection/master/ML/Pytorch/more_advanced/image_captioning/test_examples/dog.jpg\n",
        "!wget https://raw.githubusercontent.com/aladdinpersson/Machine-Learning-Collection/master/ML/Pytorch/more_advanced/image_captioning/test_examples/horse.png"
      ],
      "metadata": {
        "id": "vWtMn75htcTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Dataloader for loading the FLICKR 8k dataset and preparing it for training and testing"
      ],
      "metadata": {
        "id": "dDh9td2f-Im3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "A66NdVnWuWCv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_eng = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "Gih9tyHp-jwa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "  def __init__(self,freq_thresh):\n",
        "    self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "    self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "    self.freq_thresh = freq_thresh\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.itos)\n",
        "  \n",
        "  @staticmethod\n",
        "  def tokenizer_eng(text):\n",
        "    return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "  \n",
        "  def build_vocabulary(self,sentence_list):\n",
        "    frequencies = {}\n",
        "    idx = 4\n",
        "\n",
        "    for sentence in sentence_list:\n",
        "      for word in self.tokenizer_eng(sentence):\n",
        "        if word not in frequencies:\n",
        "          frequencies[word] = 1\n",
        "        else:\n",
        "          frequencies[word] += 1\n",
        "        \n",
        "        if frequencies[word] == self.freq_thresh:\n",
        "          self.stoi[word] = idx\n",
        "          self.itos[idx] = word\n",
        "          idx = idx + 1\n",
        "  \n",
        "  def numericalize(self,text):\n",
        "    tokenized_text = self.tokenizer_eng(text)\n",
        "\n",
        "    return [\n",
        "        self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "        for token in tokenized_text\n",
        "    ]"
      ],
      "metadata": {
        "id": "uEMO4Kjp-nxL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlickrDataset(Dataset):\n",
        "  def __init__(self,root_dir,captions_file,transform = None, freq_thresh = 5):\n",
        "    self.root_dir = root_dir\n",
        "    self.df = pd.read_csv(captions_file)\n",
        "    self.transform = transform\n",
        "\n",
        "    self.imgs = self.df[\"image\"]\n",
        "    self.captions = self.df['caption']\n",
        "\n",
        "    self.vocab = Vocabulary(freq_thresh)\n",
        "    self.vocab.build_vocabulary(self.captions.tolist())\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "  \n",
        "  def __getitem__(self,index):\n",
        "    caption = self.captions[index]\n",
        "    img_id = self.imgs[index]\n",
        "    img = Image.open(os.path.join(self.root_dir,img_id)).convert(\"RGB\")\n",
        "\n",
        "    if self.transform is not None:\n",
        "      img = self.transform(img)\n",
        "    \n",
        "    numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
        "    numericalized_caption += self.vocab.numericalize(caption)\n",
        "    numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
        "\n",
        "    return img, torch.tensor(numericalized_caption)"
      ],
      "metadata": {
        "id": "Q2u7DUYyBAHB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCollate:\n",
        "  def __init__(self,pad_idx):\n",
        "    self.pad_idx = pad_idx\n",
        "  \n",
        "  def __call__(self,batch):\n",
        "    imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "    imgs = torch.cat(imgs,dim = 0)\n",
        "    targets = [item[1] for item in batch]\n",
        "    targets = pad_sequence(targets,batch_first = False, padding_value = self.pad_idx)\n",
        "\n",
        "    return imgs, targets"
      ],
      "metadata": {
        "id": "RjkfEB49CViR"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loader(root,annotation_file,transform,batch_size = 32,num_workers = 8,shuffle = True, pin_memory = True):\n",
        "  dataset = FlickrDataset(root,annotation_file,transform = transform)\n",
        "\n",
        "  pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "  loader = DataLoader(\n",
        "      dataset = dataset,\n",
        "      batch_size = batch_size,\n",
        "      num_workers = num_workers,\n",
        "      pin_memory = pin_memory,\n",
        "      shuffle = shuffle,\n",
        "      collate_fn = MyCollate(pad_idx = pad_idx)\n",
        "  )\n",
        "\n",
        "  return loader, dataset"
      ],
      "metadata": {
        "id": "-cI_5WGvDB3S"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.Resize((224,224)), transforms.ToTensor()]\n",
        ")\n",
        "\n",
        "loader, dataset = get_loader(\n",
        "    \"./flickr8k/images/\",\n",
        "    \"./flickr8k/captions.txt\",\n",
        "    transform = transform\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR7rMw-TDoSy",
        "outputId": "87c9555f-24b2-4d1d-f775-78e474bd1a35"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, (imgs, captions) in enumerate(loader):\n",
        "  print(\"Image Shape\",imgs.shape)\n",
        "  print(\"Captions Shape\",captions.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff6BgUDvERZ2",
        "outputId": "7f2453ad-484f-44ed-8d7c-c494114a06df"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Shape torch.Size([32, 3, 224, 224])\n",
            "Captions Shape torch.Size([24, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "q_6mBIjdFyl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "kjJZIE4sEcFU"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_examples(model,device,dataset):\n",
        "  transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((299, 299)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]\n",
        "    )\n",
        "  model.eval()\n",
        "  test_img1 = transform(Image.open(\"./dog.jpg\").convert(\"RGB\")).unsqueeze(0)\n",
        "  print(\"Example 1 CORRECT: Dog on a beach by the ocean\")\n",
        "  print(\n",
        "      \"Example 1 OUTPUT: \"\n",
        "      + \" \".join(model.caption_image(test_img1.to(device), dataset.vocab))\n",
        "  )\n",
        "  test_img2 = transform(\n",
        "      Image.open(\"./child.jpg\").convert(\"RGB\")\n",
        "  ).unsqueeze(0)\n",
        "  print(\"Example 2 CORRECT: Child holding red frisbee outdoors\")\n",
        "  print(\n",
        "      \"Example 2 OUTPUT: \"\n",
        "      + \" \".join(model.caption_image(test_img2.to(device), dataset.vocab))\n",
        "  )\n",
        "  test_img3 = transform(Image.open(\"./bus.png\").convert(\"RGB\")).unsqueeze(\n",
        "      0\n",
        "  )\n",
        "  print(\"Example 3 CORRECT: Bus driving by parked cars\")\n",
        "  print(\n",
        "      \"Example 3 OUTPUT: \"\n",
        "      + \" \".join(model.caption_image(test_img3.to(device), dataset.vocab))\n",
        "  )\n",
        "  test_img4 = transform(\n",
        "      Image.open(\"./boat.png\").convert(\"RGB\")\n",
        "  ).unsqueeze(0)\n",
        "  print(\"Example 4 CORRECT: A small boat in the ocean\")\n",
        "  print(\n",
        "      \"Example 4 OUTPUT: \"\n",
        "      + \" \".join(model.caption_image(test_img4.to(device), dataset.vocab))\n",
        "  )\n",
        "  test_img5 = transform(\n",
        "      Image.open(\"./horse.png\").convert(\"RGB\")\n",
        "  ).unsqueeze(0)\n",
        "  print(\"Example 5 CORRECT: A cowboy riding a horse in the desert\")\n",
        "  print(\n",
        "      \"Example 5 OUTPUT: \"\n",
        "      + \" \".join(model.caption_image(test_img5.to(device), dataset.vocab))\n",
        "  )\n",
        "  model.train()"
      ],
      "metadata": {
        "id": "z5vicTN9KVRI"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(state,file_name = 'model_checkpoint.pth.tar'):\n",
        "  print(\"************* Saving Model Checkpoint ***************************\")\n",
        "  torch.save(state,file_name)"
      ],
      "metadata": {
        "id": "imLC5scbKozO"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "  print(\"************ Loading Checkpoint ************\")\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "  step = checkpoint['step']\n",
        "  return step"
      ],
      "metadata": {
        "id": "hHZXyevmP9_s"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Model"
      ],
      "metadata": {
        "id": "zqZ9HEAPf6aU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import statistics\n",
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "ob8NSJVNQdpj"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "\n",
        "  def __init__(self,embed_size,train_cnn = False):\n",
        "    super(EncoderCNN,self).__init__()\n",
        "\n",
        "    self.train_cnn = train_cnn\n",
        "    self.inception = models.inception_v3(pretrained = True, aux_logits = True)\n",
        "    self.inception.fc = nn.Linear(self.inception.fc.in_features,embed_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.times = []\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "  def forward(self,images):\n",
        "    features = self.inception(images)\n",
        "    return self.dropout(self.relu(features[0]))"
      ],
      "metadata": {
        "id": "AFX_3uFOgDW0"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "  \n",
        "  def __init__(self,embed_size,hidden_size,vocab_size,num_layers):\n",
        "    super(DecoderRNN,self).__init__()\n",
        "    self.embed =  nn.Embedding(vocab_size,embed_size)\n",
        "    self.lstm = nn.LSTM(embed_size,hidden_size,num_layers)\n",
        "    self.linear = nn.Linear(hidden_size,vocab_size)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "  \n",
        "  def forward(self,features,captions):\n",
        "    embeddings = self.dropout(self.embed(captions))\n",
        "    embeddings = torch.cat((features.unsqueeze(0),embeddings),dim = 0)\n",
        "    hiddens, _ = self.lstm(embeddings)\n",
        "    outputs = self.linear(hiddens)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "fLxb9M4pg39m"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNtoRNN(nn.Module):\n",
        "\n",
        "  def __init__(self,embed_size,hidden_size,vocab_size,num_layers):\n",
        "    super(CNNtoRNN,self).__init__()\n",
        "    self.encoderCNN = EncoderCNN(embed_size)\n",
        "    self.decoderRNN = DecoderRNN(embed_size,hidden_size,vocab_size,num_layers)\n",
        "    \n",
        "  def forward(self,images,captions):\n",
        "    features = self.encoderCNN(images)\n",
        "    outputs = self.decoderRNN(features,captions)\n",
        "    return outputs\n",
        "  \n",
        "  def caption_image(self,image,vocabulary,max_len = 50):\n",
        "    result_caption = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      x = self.encoderCNN(image).unsqueeze(0)\n",
        "      states = None\n",
        "\n",
        "      for _ in range(max_len):\n",
        "        hiddens, states = self.decoderRNN.lstm(x,states)\n",
        "        output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
        "        predicted = output.argmax(0)\n",
        "        result_caption.append(predicted.item())\n",
        "        x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
        "        if vocabulary.itos[predicted.item()] == '<EOS>':\n",
        "          break\n",
        "    \n",
        "    return [vocabulary.itos[idx] for idx in result_caption]"
      ],
      "metadata": {
        "id": "3cbFwN69hyD1"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "WpmfC0ZTj3FM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "Yn0FzejJjeFU"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "  transform = transforms.Compose(\n",
        "      [\n",
        "        transforms.Resize((356,356)),\n",
        "        transforms.RandomCrop((299,299)),\n",
        "       transforms.ToTensor(),\n",
        "       transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  train_loader, dataset = get_loader(\n",
        "      root = './flickr8k/images/',\n",
        "      annotation_file = './flickr8k/captions.txt',\n",
        "      transform = transform,\n",
        "      num_workers = 2\n",
        "  )\n",
        "\n",
        "  torch.backends.cudnn.benchmark = True\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  load_model = False\n",
        "  save_model = True\n",
        "  train_cnn = False\n",
        "\n",
        "  embed_size = 256\n",
        "  hidden_size = 256\n",
        "  vocab_size = len(dataset.vocab)\n",
        "  num_layers = 1\n",
        "  learning_rate = 3e-4\n",
        "  num_epochs = 20\n",
        "\n",
        "\n",
        "  writer = SummaryWriter(\"./runs/flickr\")\n",
        "  step = 0\n",
        "  model = CNNtoRNN(embed_size,hidden_size,vocab_size,num_layers).to(device)\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index = dataset.vocab.stoi[\"<PAD>\"])\n",
        "  optimizer = optim.Adam(model.parameters(),lr = learning_rate)\n",
        "\n",
        "  for name, param in model.encoderCNN.inception.named_parameters():\n",
        "    if \"fc.weight\" in name or \"fc.bias\" in name:\n",
        "      param.requires_grad = True\n",
        "    else:\n",
        "      param.required_grad = train_cnn\n",
        "    \n",
        "  if load_model:\n",
        "    step = load_checkpoint(torch.load(\"model_checkpoint.pth.tar\"),model,optimizer)\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    print_examples(model, device, dataset)\n",
        "\n",
        "    print(\"<============Epoch=============>\",epoch)\n",
        "    if save_model:\n",
        "      checkpoint = {\n",
        "          \"state_dict\": model.state_dict(),\n",
        "          \"optimizer\": optimizer.state_dict(),\n",
        "      }\n",
        "      save_checkpoint(checkpoint)\n",
        "\n",
        "    for idx, (imgs,captions) in tqdm(enumerate(train_loader), total = len(train_loader), leave = False):\n",
        "      imgs = imgs.to(device)\n",
        "      captions = captions.to(device)\n",
        "      outputs = model(imgs,captions[:-1])\n",
        "      loss = criterion(outputs.reshape(-1,outputs.shape[2]),captions.reshape(-1))\n",
        "\n",
        "      writer.add_scalar(\"Training Loss\",loss.item(),global_step = step)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward(loss)\n",
        "      optimizer.step()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "WwVEBRnmlW7q"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2tASUyhtAZc",
        "outputId": "8b5a0ddf-4707-4626-f796-a9c2757b3ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1 CORRECT: Dog on a beach by the ocean\n",
            "Example 1 OUTPUT: padded teacher creating wine gold strange rowing puppy skimpy lasso that alongside small followed electronic touching runners outside snowy milkshake vine lunch self pugs for wine wires apples wrapping gravel squirted naked attention gathers creating wine gold strange rowing puppy skimpy lasso that alongside small followed electronic touching runners outside\n",
            "Example 2 CORRECT: Child holding red frisbee outdoors\n",
            "Example 2 OUTPUT: victory mug men pouring footballers headset offering onlookers batting lining cones unhappy mug men travels formally pet spot four color teacher tiny cannon field caps marble interested still padded crosswalk fight dragging to grey dive big instrument ten when struggles climbers refrigerator climbers refrigerator squeezing lunch dimly converse patio checkered\n",
            "Example 3 CORRECT: Bus driving by parked cars\n",
            "Example 3 OUTPUT: victory mug tries connected shown cups clouds rollerblader unhappy motorcyclists individual big hamburgers type yard skateboarder misty khaki jogger try position corgi gear asian float columns windsurfs hell converse forward identical diving films stands stands stands stands stands have umbrella interacting number exhibit own footprints protest toy toy jumping blurry\n",
            "Example 4 CORRECT: A small boat in the ocean\n",
            "Example 4 OUTPUT: selling legs chest hopping balance matching matching we cones connected shown track mug men close medium victory floats sad crane posing they staring low tourists low darkened observed wheeled tiny farm ears sport homemade mess congregate rolls shops backpacking earphones seattle lining cones unhappy dogs bubbles earrings gear doorway string\n",
            "Example 5 CORRECT: A cowboy riding a horse in the desert\n",
            "Example 5 OUTPUT: selling legs chest hopping balance matching matching we cones connected shown track mug men close medium victory floats sad crane posing they staring low tourists low darkened observed wheeled tiny farm ears sport homemade mess congregate rolls shops backpacking earphones seattle lining cones unhappy dogs bubbles earrings gear doorway string\n",
            "<============Epoch=============> 0\n",
            "************* Saving Model Checkpoint ***************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▎        | 171/1265 [01:23<08:20,  2.18it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fxxLLHCobIyr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}